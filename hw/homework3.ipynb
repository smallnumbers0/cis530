{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VazAtArX038t"
   },
   "source": [
    "# Homework 3: N-Gram Language Models\n",
    "\n",
    "## Total Points: 95 + 5 bonus\n",
    "- **Overview**: In the textbook, language modeling was defined as the task of predicting the next word in a sequence given the previous words. In this assignment, we will focus on the related problem of predicting the next character in a sequence given the previous characters.\n",
    "\n",
    "- **Learning goals**:\n",
    "    - Understand how to compute language model probabilities using maximum likelihood estimation.\n",
    "    - Implement basic smoothing, back-off and interpolation.\n",
    "    - Have fun using a language model to probabilistically generate texts.\n",
    "    - Use a set of language models to perform text classification.\n",
    "\n",
    "- **Data**: We will provide you with training and development data that has been manually labeled. We will also give you a test set without labels. You will build a classifier to predict the labels on our test set. You can upload your classifier’s predictions to Gradescope. We will score its predictions and maintain a leaderboard showing whose classifier has the best performance.\n",
    "\n",
    "- **Delieverables:** This assignment has several deliverables:\n",
    "    - Your implementations for the functions in the skeleton code (this notebook)\n",
    "    - Your model’s output for the test set (your model will be ranked on a leaderboard against the other students’ outputs)\n",
    "\n",
    "- **Grading**: We will use the auto-grading system called `PennGrader`. To complete the homework assignment, you should implement anything marked with `#TODO` and run the cell with `#PennGrader` note. **There will be no hidden tests in this assignment.** In other words, you will know your score once you finish all the `#TODO` and run all the `#PennGrader` tests!\n",
    "\n",
    "\n",
    "## Recommended Readings\n",
    "- [Language Modeling with N-grams](https://web.stanford.edu/~jurafsky/slp3/3.pdf). Dan Jurafsky and James H. Martin. Speech and Language Processing (3rd edition draft) .\n",
    "- [A Bit of Progress in Language Modeling](https://arxiv.org/abs/cs/0108005). Joshua Goodman. Computer Speech and Language .\n",
    "- [The Unreasonable Effectiveness of Character-level Language Models](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139). Yoav Goldberg. Response to Andrej Karpathy's blog post. 2015.\n",
    "- [Language Independent Authorship Attribution using Character Level Language Models](http://www.aclweb.org/anthology/E/E03/E03-1053.pdf). Fuchun Pen, Dale Schuurmans, Vlado Keselj, Shaojun Wan. EACL 2003.\n",
    "\n",
    "## To get started, **make a copy** of this colab notebook into your google drive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDv9qN5c9357"
   },
   "source": [
    "## Setup 1: PennGrader Setup [4 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jpSjR2N19kk4"
   },
   "outputs": [],
   "source": [
    "## DO NOT CHANGE ANYTHING, JUST RUN\n",
    "%%capture\n",
    "!pip install penngrader-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvPA8Z2D9ki_",
    "outputId": "88d77b2a-7983-45ec-8d13-851d7a2467c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notebook-config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile notebook-config.yaml\n",
    "\n",
    "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
    "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbeXT4Oj9kg_",
    "outputId": "f129aaa9-6faf-4cee-c8aa-541d4bab5123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
      "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'\n"
     ]
    }
   ],
   "source": [
    "!cat notebook-config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OceP0Hr-9kfC",
    "outputId": "96178b25-5e47-425e-fc5d-0b78ec1be278"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PennGrader initialized with Student ID: 56803282\n",
      "\n",
      "Make sure this correct or we will not be able to store your grade\n"
     ]
    }
   ],
   "source": [
    "from penngrader.grader import *\n",
    "\n",
    "## TODO - Start\n",
    "STUDENT_ID = 56803282 # YOUR PENN-ID GOES HERE AS AN INTEGER#\n",
    "## TODO - End\n",
    "\n",
    "SECRET = STUDENT_ID\n",
    "grader = PennGrader('notebook-config.yaml', 'CIS5300_OL_23Su_HW3', STUDENT_ID, SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5lAgKaYa-uN",
    "outputId": "9dfe24b4-6357-4639-aa0c-3608c24b197e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 4/4 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# check if the PennGrader is set up correctly\n",
    "# do not chance this cell, see if you get 4/4!\n",
    "name_str = 'Jacky Choi'\n",
    "grader.grade(test_case_id = 'name_test', answer = name_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHeB9GLB-PO1"
   },
   "source": [
    "## Setup 2: Dataset / Packages\n",
    "- **Run the following cells without changing anything!**\n",
    "- [Loading dataset from huggingface](https://huggingface.co/docs/datasets/v1.8.0/loading_datasets.html#from-local-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "E6qSD12uEx4T"
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "from collections import Counter\n",
    "from dill.source import getsource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "hhCl4yTEUB2v"
   },
   "outputs": [],
   "source": [
    "## DO NOT CHANGE ANYTHING, JUST RUN\n",
    "\n",
    "def get_class_source(cls):\n",
    "    import re\n",
    "    class_name = cls.__name__\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()\n",
    "    inputs = ipython.user_ns['In']\n",
    "    pattern = re.compile(r'^\\s*class\\s+{}\\b'.format(class_name))\n",
    "    for cell in reversed(inputs):\n",
    "        if pattern.search(cell):\n",
    "            return cell\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iosTnYSebMK5",
    "outputId": "48db5954-665f-42ee-e441-9b97858cd9e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=19ZFc_nFF3T-LTI9rxXUy1XyMaoGSzN3T\n",
      "To: /content/cities_train.zip\n",
      "\r  0% 0.00/160k [00:00<?, ?B/s]\r100% 160k/160k [00:00<00:00, 24.7MB/s]\n",
      "Archive:  cities_train.zip\n",
      "  inflating: train/af.txt            \n",
      "  inflating: train/de.txt            \n",
      "  inflating: train/fi.txt            \n",
      "  inflating: train/fr.txt            \n",
      "  inflating: train/in.txt            \n",
      "  inflating: train/ir.txt            \n",
      "  inflating: train/cn.txt            \n",
      "  inflating: train/za.txt            \n",
      "  inflating: train/pk.txt            \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1BH9qhvR7POqmGPOPadqADv-NxdGBgyzu\n",
      "To: /content/cities_val.zip\n",
      "100% 7.56k/7.56k [00:00<00:00, 21.6MB/s]\n",
      "Archive:  cities_val.zip\n",
      "  inflating: val/af.txt              \n",
      "  inflating: val/cn.txt              \n",
      "  inflating: val/de.txt              \n",
      "  inflating: val/fi.txt              \n",
      "  inflating: val/fr.txt              \n",
      "  inflating: val/ir.txt              \n",
      "  inflating: val/za.txt              \n",
      "  inflating: val/pk.txt              \n",
      "  inflating: val/in.txt              \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1mz6EdH-4pOwr-o3lHDlh1SS7oRdydNr1\n",
      "To: /content/cities_test.txt\n",
      "100% 11.0k/11.0k [00:00<00:00, 25.1MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=11es6r9fVz_wf_Q2T7oP2IQDkAAmhtzf4\n",
      "To: /content/nytimes_article.txt\n",
      "100% 4.71k/4.71k [00:00<00:00, 13.4MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=19P37gR_XDup-h3a4LdKtQWbdM-lGPqin\n",
      "To: /content/shakespeare_sonnets.txt\n",
      "100% 9.40k/9.40k [00:00<00:00, 19.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1kKfh2oaxinBPVwWaux6FA9TerqAH5nHw\n",
      "To: /content/shakespeare_input.txt\n",
      "100% 4.57M/4.57M [00:00<00:00, 191MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 19ZFc_nFF3T-LTI9rxXUy1XyMaoGSzN3T # https://drive.google.com/file/d/19ZFc_nFF3T-LTI9rxXUy1XyMaoGSzN3T/view?usp=share_link\n",
    "!unzip -o cities_train.zip\n",
    "!gdown 1BH9qhvR7POqmGPOPadqADv-NxdGBgyzu # https://drive.google.com/file/d/1BH9qhvR7POqmGPOPadqADv-NxdGBgyzu/view?usp=sharing\n",
    "!unzip -o cities_val.zip\n",
    "!gdown 1mz6EdH-4pOwr-o3lHDlh1SS7oRdydNr1 # https://drive.google.com/file/d/1mz6EdH-4pOwr-o3lHDlh1SS7oRdydNr1/view?usp=sharing\n",
    "!gdown 11es6r9fVz_wf_Q2T7oP2IQDkAAmhtzf4 # https://drive.google.com/file/d/11es6r9fVz_wf_Q2T7oP2IQDkAAmhtzf4/view?usp=share_link\n",
    "!gdown 19P37gR_XDup-h3a4LdKtQWbdM-lGPqin # https://drive.google.com/file/d/19P37gR_XDup-h3a4LdKtQWbdM-lGPqin/view?usp=sharing\n",
    "!gdown 1kKfh2oaxinBPVwWaux6FA9TerqAH5nHw # https://drive.google.com/file/d/1kKfh2oaxinBPVwWaux6FA9TerqAH5nHw/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcYJEzt62DMM"
   },
   "source": [
    "# Section 0: Generating N-Grams [5 points]\n",
    "- **Problem 0:** Write a function `ngrams(n, text)` that produces a list of all n-grams of the specified size from the input text. Each n-gram should consist of a 2-element tuple `(context, char)`, where the context is itself an n-length string comprised of the n characters preceding the current character. The sentence should be padded with n ~ characters at the beginning (we’ve provided you with `start_pad(n)` for this purpose). If n=0 , all contexts should be empty strings. You may assume that n≥0.\n",
    "\n",
    "```\n",
    ">>> ngrams(1, 'abc')\n",
    "[('~', 'a'), ('a', 'b'), ('b', 'c')]\n",
    "\n",
    ">>> ngrams(2, 'abc')\n",
    "[('~~', 'a'), ('~a', 'b'), ('ab', 'c')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Mp4f8Cdl0zqz"
   },
   "outputs": [],
   "source": [
    "### DO NOT CHANGE ###\n",
    "def start_pad(n):\n",
    "    ''' Returns a padding string of length n to append to the front of text\n",
    "        as a pre-processing step to building n-grams '''\n",
    "    return '~' * n\n",
    "### DO NOT CHANGE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "UkB6aErq0zo1"
   },
   "outputs": [],
   "source": [
    "def ngrams(n, text):\n",
    "    ''' Returns the ngrams of the text as tuples where the first element is\n",
    "        the length-n context and the second is the character '''\n",
    "\n",
    "    ## AUTOGRADER HELPER ##\n",
    "    def start_pad(n):\n",
    "        ''' Returns a padding string of length n to append to the front of text\n",
    "            as a pre-processing step to building n-grams '''\n",
    "        return '~' * n\n",
    "    ## AUTOGRADER HELPER ##\n",
    "\n",
    "\n",
    "\n",
    "    ## TODO: create n-grams given the text\n",
    "    #>>> ngrams(1, 'abc')\n",
    "    #[('~', 'a'), ('a', 'b'), ('b', 'c')]\n",
    "    # >>> ngrams(2, 'abc')\n",
    "    # [('~~', 'a'), ('~a', 'b'), ('ab', 'c')]\n",
    "    ngrams_arr = []\n",
    "    padding = start_pad(n) + text\n",
    "\n",
    "    for i in range(len(text)):\n",
    "        context = padding[i:i+n]\n",
    "        char = text[i]\n",
    "        ngrams_arr.append((context, char))\n",
    "\n",
    "    return ngrams_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-QvBVDNzJXz",
    "outputId": "bfea44e2-ebd6-4898-d7de-f974af1da35d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('~', 'a'), ('a', 'b'), ('b', 'c')]\n",
      "[('~~', 'a'), ('~a', 'b'), ('ab', 'c')]\n"
     ]
    }
   ],
   "source": [
    "print(ngrams(1, 'abc'))\n",
    "print(ngrams(2, 'abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ottMfhYnNhB",
    "outputId": "8da86b32-5538-4bef-e73a-8be9d87569b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 5/5 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# PennGrader - DO NOT CHANGE\n",
    "grader.grade(test_case_id = 'test_q0_ngrams', answer = getsource(ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK6hMY3ZdbtG"
   },
   "source": [
    "# Section 1: Creating an N-Gram Model [36 points]\n",
    "In this section, you will build a simple n-gram language model that can be used to generate random text resembling a source document. Your use of external code should be limited to built-in Python modules, which excludes, for example, `NumPy` and `NLTK`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "wrApPRz6d1E5"
   },
   "outputs": [],
   "source": [
    "class NgramModel(object):\n",
    "    ''' A basic n-gram model using add-k smoothing '''\n",
    "\n",
    "    def __init__(self, n, k = 0):\n",
    "        # About `k`: you don't need to worry about it for problem 1.1, it is something you will need in problem 2.1\n",
    "        self.k = k\n",
    "        # contexts is a dictionary with the context as keys and corresponding value is a dictionary of count and\n",
    "        # chars which is a dictionary of characters as key and it's count as value.\n",
    "        # Please do not change the format of storing the counts and characters, the autograder uses this structure\n",
    "        # For example:\n",
    "        # if the context is 'ab' with count=3 and characters following it are 'a' and 'b' with counts 2 and 1 respectively\n",
    "        # then the context dictionary would look like this:\n",
    "        # {\n",
    "        #   'ab' : {'count': 3,'chars': {'a' : 2, 'b' : 1}},\n",
    "        #   'aa' : {...},\n",
    "        #   ...\n",
    "        # }\n",
    "        self.contexts = {}\n",
    "\n",
    "        ## TODO 1.1 - see problem 1.1\n",
    "        self.n = n\n",
    "        self.vocab = set()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        ''' Returns the set of characters in the vocab '''\n",
    "        ## TODO 1.1 - see problem 1.1\n",
    "        return self.vocab\n",
    "\n",
    "    def get_context(self):\n",
    "        ''' Returns the dictionary for context '''\n",
    "        return self.contexts\n",
    "\n",
    "\n",
    "    '''Write a method update(self, text) which computes the n-grams for the input sentence and updates the internal\n",
    "    counts. Also write a method prob(self, context, char) which accepts an n-length string representing a context and\n",
    "    a character, and returns the probability of that character occuring, given the preceding context. If you encounter\n",
    "    a novel context, the probability of any given char should be 1/V where V is the size of the vocab.'''\n",
    "    def update(self, text):\n",
    "        ''' Updates the model n-grams based on text '''\n",
    "        ## TODO 1.2 - see problem 1.2\n",
    "        ngrams_arr = ngrams(self.n, text) #full list of ngram tuples(context, char)\n",
    "        for context, char in ngrams_arr:\n",
    "            if context not in self.contexts:\n",
    "                self.contexts[context] = {'count': 0, 'chars': {}}\n",
    "            #increment count of countext by 1\n",
    "            self.contexts[context]['count'] += 1\n",
    "            #captures char after context\n",
    "            if char not in self.contexts[context]['chars']:\n",
    "                self.contexts[context]['chars'][char] = 0\n",
    "            #increment this char by 1\n",
    "            self.contexts[context]['chars'][char] += 1\n",
    "            #add to the vocab set\n",
    "            self.vocab.add(char)\n",
    "\n",
    "    def prob(self, context, char):\n",
    "        ''' Returns the probability of char appearing after context '''\n",
    "\n",
    "        if self.k == 0:\n",
    "\n",
    "            if context in self.contexts:\n",
    "                if char in self.contexts[context]['chars']:\n",
    "                    return self.contexts[context]['chars'][char] / self.contexts[context]['count']\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                return 1 / len(self.vocab) if len(self.vocab) > 0 else 0\n",
    "        else:\n",
    "        # TODO: Implement add-k smoothing here (for self.k > 0)\n",
    "\n",
    "            if context in self.contexts:\n",
    "                numerator = self.contexts[context]['chars'].get(char, 0) + self.k\n",
    "                denominator = self.contexts[context]['count'] + (self.k * len(self.vocab))\n",
    "                return numerator / denominator\n",
    "            else:\n",
    "\n",
    "                return self.k / (self.k * len(self.vocab)) if len(self.vocab) > 0 else 0\n",
    "\n",
    "\n",
    "    def random_char(self, context):\n",
    "        ''' Returns a random character based on the given context and the\n",
    "            n-grams learned by this model '''\n",
    "        ## TODO 1.3 - see problem 1.3\n",
    "        #Write a method random_char(self, context) which returns a random character according to the probability distribution determined by the given context. Specifically, let  𝑉=⟨𝑣1,𝑣2,⋯,𝑣𝑛⟩  be the vocab, sorted according to Python's natural lexicographic ordering, and let  𝑟  be a random number between 0 and 1. Your method should return the character  𝑣𝑖  such that:\n",
    "        r = random.random() #0 - 1\n",
    "        prev_sum = 0\n",
    "        for i, char in enumerate(sorted(self.vocab)):\n",
    "            post_sum = prev_sum + self.prob(context, char)\n",
    "            if prev_sum < r < post_sum:\n",
    "                return char\n",
    "            prev_sum = post_sum\n",
    "\n",
    "\n",
    "\n",
    "    def random_text(self, length):\n",
    "        ''' Returns text of the specified character length based on the\n",
    "            n-grams learned by this model '''\n",
    "        ## TODO 1.4 - see problem 1.4\n",
    "        context = start_pad(self.n)\n",
    "        text = ''\n",
    "        for _ in range(length):\n",
    "            char = self.random_char(context)\n",
    "            text += char\n",
    "            context = context[1:] + char\n",
    "        return text\n",
    "\n",
    "    def perplexity(self, text):\n",
    "        ''' Returns the perplexity of text based on the n-grams learned by\n",
    "            this model '''\n",
    "        ## TODO 2.2 - see problem 2.2\n",
    "\n",
    "        length_text = len(text)\n",
    "        log_prob = 0\n",
    "        ngrams_arr = ngrams(self.n, text)\n",
    "        for context, char in ngrams_arr:\n",
    "           prob = self.prob(context,char)\n",
    "           if prob == 0:\n",
    "              return float('inf')\n",
    "           log_prob += math.log(prob)\n",
    "        perplexity = math.exp(-1/length_text * log_prob)\n",
    "        return perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWI1aiMPdndY"
   },
   "source": [
    "## 1.1 Initialization [3 points]\n",
    "In the `NgramModel` class, write an initialization method `__init__(self, n, k)` which stores the order n of the model and initializes any necessary internal variables. Then write a method `get_vocab(self)` that returns the vocab (this is the `set` of all characters used by this model).\n",
    "\n",
    "- **Problem 1.1:** finish `__init__(self, n, k)` and `get_vocab(self)` [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GToeslUHoh68",
    "outputId": "f6a841fc-b11d-410a-b176-86c32db3057b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 3/3 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# PennGrader - DO NOT CHANGE\n",
    "ngram_model = NgramModel(1, 0)\n",
    "grader.grade(test_case_id = 'test_q11_init_get_vocab', answer = get_class_source(NgramModel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLaGXAE8eBRY"
   },
   "source": [
    "## 1.2 Update and Calculate Probabilities [15 points]\n",
    "Write a method `update(self, text)` which computes the n-grams for the input sentence and updates the internal counts. Also write a method `prob(self, context, char)` which accepts an n-length string representing a context and a character, and returns the probability of that character occuring, given the preceding context. If you encounter a novel `context`, the probability of any given `char` should be 1/V where V is the size of the vocab.\n",
    "\n",
    "- **Problem 1.2:** finish `update(self, text)` and `prob(self, context, char)` [15 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrfSJuQ00x2S",
    "outputId": "2e5307a9-1048-4df1-c3b4-1b100afba48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 15/15 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# PennGrader - DO NOT CHANGE\n",
    "all_test_cases = [\n",
    "    (1, ['abab', 'abcd'], [('b','c'), ('a', '~')]),\n",
    "    (2, ['ababa', 'abcdddd', 'babcdabag'], [('ab', 'a'), ('ad', 'b')]),\n",
    "    (3, ['ababa', 'abcdddd', 'babcdabag'], [('ddd', 'd'), ('~~c', 'f')])\n",
    "]\n",
    "\n",
    "all_test_results = []\n",
    "for test_data in all_test_cases:\n",
    "    nm = NgramModel(test_data[0], 0)\n",
    "    for text in test_data[1]:\n",
    "        nm.update(text)\n",
    "    vocab = nm.get_vocab()\n",
    "    probs = []\n",
    "    for prob_check in test_data[2]:\n",
    "        prob = nm.prob(prob_check[0], prob_check[1])\n",
    "        probs.append(prob)\n",
    "    all_test_results.append((vocab, probs))\n",
    "\n",
    "# reload_grader()\n",
    "grader.grade(test_case_id = 'test_q12_update_prob', answer = all_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3J4cV0liozJ"
   },
   "source": [
    "## 1.3 Random Character [6 points]\n",
    "\n",
    "Write a method `random_char(self, context)` which returns a random character according to the probability distribution determined by the given context. Specifically, let $V=\\langle v_1,v_2, \\cdots, v_n \\rangle$ be the vocab, sorted according to Python's natural lexicographic ordering, and let $r$ be a random number between 0 and 1. Your method should return the character $v_i$ such that:\n",
    "$$\n",
    "\\sum_{j=1}^{i-1} P(v_j\\ |\\ \\text{context}) \\le r < \\sum_{j=1}^i P(v_j\\ | \\ \\text{context}).\n",
    "$$\n",
    "You should use a single call to the `random.random()` function to generate $r$, for example:\n",
    "```\n",
    ">>> m = NgramModel(0, 0)\n",
    ">>> m.update('abab')\n",
    ">>> m.update('abcd')\n",
    ">>> random.seed(1)\n",
    ">>> [m.random_char('') for i in range(25)]\n",
    "['a', 'c', 'c', 'a', 'b', 'b', 'b', 'c', 'a', 'a', 'c', 'b', 'c', 'a', 'b', 'b', 'a', 'd', 'd', 'a', 'a', 'b', 'd', 'b', 'a']\n",
    "```\n",
    "\n",
    "- **Problem 1.3:** finish `random_char(self, context)` [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoBV14Hh54Z7",
    "outputId": "b26fa74e-33ae-4410-e965-83ed197c846a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 6/6 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# PennGrader - DO NOT CHANGE\n",
    "corpus = ['ababa', 'abcdddd', 'babcdabag', 'dbaa', 'dbab']\n",
    "nm = NgramModel(1, 0)\n",
    "for text in corpus:\n",
    "    nm.update(text)\n",
    "\n",
    "random_seed_lst = [2, 3, 4]\n",
    "random_char_lst = []\n",
    "for seed in random_seed_lst:\n",
    "    random.seed(seed)\n",
    "    random_char = [nm.random_char('') for i in range(25)]\n",
    "    random_char_lst.append(random_char)\n",
    "\n",
    "grader.grade(test_case_id = 'test_q13_rand_char', answer = random_char_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScFUxj0okFa2"
   },
   "source": [
    "## 1.4 Random Text [6 points]\n",
    "In the `NgramModel` class, write a method `random_text(self, length)` which returns a string of characters chosen at random using the `random_char(self, context)` method. Your starting context should always be $n$ ~ characters, and the context should be updated as characters are generated. If $n=0$, your context should always be the empty string. You should continue generating characters until you've produced the specified number of random characters, then return the full string.\n",
    "\n",
    "```python\n",
    ">>> m = NgramModel(1, 0)\n",
    ">>> m.update('abab')\n",
    ">>> m.update('abcd')\n",
    ">>> random.seed(1)\n",
    ">>> m.random_text(25)\n",
    "abcdbabcdabababcdddabcdba\n",
    "```\n",
    "\n",
    "- **Problem 1.4:** finish `random_text(self, length)` [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDEHJ-y4iocV",
    "outputId": "6a3dd6b7-81a9-4376-dda0-3af76d560fcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 6/6 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# PennGrader - DO NOT CHANGE\n",
    "corpus = ['ababa', 'abcdddd', 'babcdabag', 'dbaa', 'dbab']\n",
    "nm = NgramModel(1, 0)\n",
    "for text in corpus:\n",
    "    nm.update(text)\n",
    "\n",
    "random_seed_lst = [42, 43, 44]\n",
    "random_text_lst = []\n",
    "for seed in random_seed_lst:\n",
    "    random.seed(seed)\n",
    "    random_text = nm.random_text(25)\n",
    "    random_text_lst.append(random_text)\n",
    "\n",
    "grader.grade(test_case_id = 'test_q14_rand_text', answer = random_text_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFj3Zb5zkWbA"
   },
   "source": [
    "## 1.5 Writing Shakespeare [6 points]\n",
    "\n",
    "Now you can train a language model. We have provided this corpus of Shakespeare at `shakespeare_input.txt` (if you cannot find it, re-run the `Setup 2: Dataset / Packages` section).\n",
    "\n",
    "We’ve also given you the function `create_ngram_model(model_class, path, n, k)` that will create and return an n-gram model trained on the entire file path provided and `create_ngram_model_lines(model_class, path, n, k)` that will create and return an n-gram model trained line-by-line on the file path provided. You should use the first for the Shakespeare file and the second for the city name files.\n",
    "\n",
    "Try generating some Shakespeare with different order n-gram models. You should try running the following commands:\n",
    "\n",
    "```python\n",
    ">>> m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 2)\n",
    ">>> m.random_text(250)\n",
    "\n",
    ">>> m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 3)\n",
    ">>> m.random_text(250)\n",
    "\n",
    ">>> m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 4)\n",
    ">>> m.random_text(250)\n",
    "\n",
    ">>> m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 7)\n",
    ">>> m.random_text(250)\n",
    "```\n",
    "\n",
    "What do you think? Is it as good as [1000 monkeys working at 1000 typewriters](https://www.youtube.com/watch?v=no_elVGGgW8)?\n",
    "\n",
    "After generating a bunch of short passages, do you notice anything? *They all start with F!* In fact, after we hit a certain order, the first word is always *First*?  Why is that? Is the model trying to be clever? Is it saying *First, I will generate the word \"First\"*?  No, probably not.  Explain what is going on in your writeup.\n",
    "\n",
    "- **Answer 1.5:** Generate Shakespear texts with at least 3 numbers of n and discuss on the results [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlJCF_2FAMum"
   },
   "source": [
    "After looking at Shakespeare's input text file, I found that the word First was very common in beginning a dialogue for his writings as well as many names which also appear to be predicted by the models. I believe the ngram model training could have adopted the Shakespearian writing style based on the predictions, patterns and probability that these words would appear. Some of the most common phrases include First ANY_NAME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "0pQlbgsfm18j"
   },
   "outputs": [],
   "source": [
    "## DO NOT CHANGE ##\n",
    "def create_ngram_model(model_class, path, n=2, k=0):\n",
    "    ''' Creates and returns a new n-gram model trained on the city names\n",
    "        found in the path file '''\n",
    "    model = model_class(n, k)\n",
    "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        model.update(f.read())\n",
    "    return model\n",
    "\n",
    "def create_ngram_model_lines(model_class, path, n=2, k=0):\n",
    "    ''' Creates and returns a new n-gram model trained on the city names\n",
    "        found in the path file '''\n",
    "    model = model_class(n, k)\n",
    "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            model.update(line.strip())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "X-UIxKXjm16V",
    "outputId": "a2abe3d9-f4ac-4606-b59e-b5ff64c5110a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'First Citizen:\\nThis is the sad and we been impostor that sets up\\nhis head, and Warwick knew it all of good?\\n\\nOCTAVIUS CAESAR:\\nI must not his\\nname out such a lose and sudden command,\\nIf crooked sadly themselves alone?\\n\\nLUCIUS:\\nMad for the powers? show'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 7)\n",
    "m.random_text(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "fy00HFxf-lZ2",
    "outputId": "0c422737-ea09-4279-da7e-0a660a6be799"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"First Soldier father for a structed: my friences, arity:\\nBut, for I will ours\\nFrom o' thee word can discorn?\\nEither heavens the well as Joan will take, whereign's hire the fear: she's proteus, all I had go:\\nCome thus.\\n\\nRODERICLES:\\nThe stress,\\nWe five\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 4)\n",
    "m.random_text(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "QPUGivRJ9L1P",
    "outputId": "0a03c4c0-2d56-4f37-d74e-89b5ae385fbd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"First to watchful stomach a night.\\nShould, Petruchio is thrive,\\nI wouldst thou are to present thy father guess bravell'd Derceth thee well! what the gentle, knight, good and that\\nHath made a sop o' t' oppress'd intervalued throw shall, Kate, what a v\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 5)\n",
    "m.random_text(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "FJUjplaDBMez",
    "outputId": "f1453cf4-4671-4ec5-8097-fd72b991ac3a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"First Senator:\\nWe are happiness.\\n\\nKING CLAUDIO:\\nUnhappy.\\n\\nCORIOLANUS:\\nWhat dost not straight;\\nFour feast and at lustier task in his loyalty!\\nHere comes.\\nThat thou see thy bladed ground, an God forbid it.\\n\\nSIR ANDREW:\\nVery man hence, and devour'd,\\nBy \""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 6)\n",
    "m.random_text(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJ439II7nFyW"
   },
   "source": [
    "# Section 2: Smoothing, Perplexity, and Interpolation [30 points]\n",
    "\n",
    "In this part of the assignment, you'll adapt your code in order to implement several of the  techniques described in [Section 3.5 of the Jurafsky and Martin textbook](https://web.stanford.edu/~jurafsky/slp3/3.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hekp9eUXmjJM"
   },
   "source": [
    "## 2.1 Smoothing [6 points]\n",
    "\n",
    "Laplace Smoothing is described in section 3.5.1 of the book. Laplace smoothing adds one to each count (hence its alternate name *add-one smoothing*). Since there are *V* characters in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.\n",
    "\n",
    "$$P_{Laplace}(w_i) = \\frac{count_i + 1}{N+|V|}$$\n",
    "\n",
    "A variant of Laplace smoothing is called *Add-k smoothing* or *Add-epsilon smoothing*. This is described in section *Add-k 3.5.2*.\n",
    "\n",
    "```python\n",
    ">>> m = NgramModel(1, 1)\n",
    ">>> m.update('abab')\n",
    ">>> m.update('abcd')\n",
    ">>> m.prob('a', 'a')\n",
    "0.14285714285714285\n",
    ">>> m.prob('a', 'b')\n",
    "0.5714285714285714\n",
    ">>> m.prob('c', 'd')\n",
    "0.4\n",
    ">>> m.prob('d', 'a')\n",
    "0.25\n",
    "```\n",
    "\n",
    "- **Problem 2.1**: Update your `NgramModel` code from Part 1 to implement add-k smoothing. (see comment of `## TODO 2.1`) [6 points]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ajK05IxoEWy",
    "outputId": "b117702a-9053-4c04-9a50-c518567988b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 6/6 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# PennGrader - DO NOT CHANGE\n",
    "smoothing_test_cases = [\n",
    "    (1, 1, ['abab', 'abcd'], ('b', 'a')),\n",
    "    (2, 1, ['ababa', 'abcdddd', 'babcdabag'], ('ba', 'b')),\n",
    "    (3, 2, ['ababa', 'abcdddd', 'babcdabag', 'dbaa', 'dbab'], ('aba', 'b'))\n",
    "]\n",
    "smoothing_test_results = []\n",
    "for i, j, corpus, test_words in smoothing_test_cases:\n",
    "    nm = NgramModel(i, j)\n",
    "    for text in corpus:\n",
    "        nm.update(text)\n",
    "    smoothing_test_results.append(nm.prob(test_words[0], test_words[1]))\n",
    "\n",
    "grader.grade(test_case_id = 'test_q21_smoothing', answer = smoothing_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPSvquemm1n3"
   },
   "source": [
    "## 2.2 Perplexity [12 points]\n",
    "\n",
    "How do we know whether a language model is good? There are two basic approaches:\n",
    "1. Task-based evaluation (also known as **extrinsic** evaluation), where we use the language model as part of some other task, like automatic speech recognition, or spelling correcktion, or an OCR system that tries to covert a professor's messy handwriting into text.\n",
    "2. **Intrinsic** evaluation: Intrinsic evaluation tries to directly evalute the goodness of the language model by seeing how well the probability distributions that it estimates are able to explain some previously unseen test set.\n",
    "\n",
    "Here's what the textbook says:\n",
    "\n",
    "> For an intrinsic evaluation of a language model we need a test set. As with many of the statistical models in our field, the probabilities of an N-gram model come from the corpus it is trained on, the training set or training corpus. We can then measure the quality of an N-gram model by its performance on some unseen data called the test set or test corpus. We will also sometimes call test sets and other datasets that are not in our training sets held out corpora because we hold them out from the training data.\n",
    "\n",
    "> So if we are given a corpus of text and want to compare two different N-gram models, we divide the data into training and test sets, train the parameters of both models on the training set, and then compare how well the two trained models fit the test set.\n",
    "\n",
    "> But what does it mean to \"fit the test set\"? The answer is simple: whichever model assigns a higher probability to the test set is a better model.\n",
    "\n",
    "We'll implement the most common method for intrinsic metric of language models: *perplexity*.  The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of characters. For a test set $W = w_1 w_2 ... w_N$:\n",
    "\n",
    "$$Perplexity(W) = P(w_1 w_2 ... w_N)^{-\\frac{1}{N}}$$\n",
    "\n",
    "$$ = \\sqrt[N]{\\frac{1}{P(w_1 w_2 ... w_N)}}$$\n",
    "\n",
    "$$ = \\sqrt[N]{\\prod_{i=1}^{N}{\\frac{1}{P(w_i \\mid w_1 ... w_{i-1})}}}$$\n",
    "\n",
    "Now implement the `perplexity(self, text)` function in `NgramModel`. A couple of things to keep in mind:\n",
    "1. Numeric underflow is going to be a problem, so consider using logs.\n",
    "2. Perplexity is undefined if the language model assigns any zero probabilities to the test set. In that case your code should return positive infinity - `float('inf')`.\n",
    "3. On your unsmoothed models, you'll definitely get some zero probabilities for the test set. To test you code, you should try computing perplexity on the training set, and you should compute perplexity for your language models that use smoothing and interpolation.\n",
    "\n",
    "```python\n",
    ">>> m = NgramModel(1, 0)\n",
    ">>> m.update('abab')\n",
    ">>> m.update('abcd')\n",
    ">>> m.perplexity('abcd')\n",
    "1.189207115002721\n",
    ">>> m.perplexity('abca')\n",
    "inf\n",
    ">>> m.perplexity('abcda')\n",
    "1.515716566510398\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA8hyM15oq0Z"
   },
   "source": [
    "- **Problem 2.2**: implement the `perplexity(self, text)` function in `NgramModel` [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Je80gVOzmi5R",
    "outputId": "cca9c008-1e63-49a7-c111-ffab61498e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 6/6 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# PennGrader - DO NOT CHANGE\n",
    "ppl_test_cases = [\n",
    "    (1, 0, ['abab', 'abcd'], 'abca'),\n",
    "    (2, 0, ['ababa', 'abcdddd', 'babcdabag'], 'ba'),\n",
    "    (3, 1, ['ababa', 'abcdddd', 'babcdabag', 'dbaa', 'dbab'], 'abcdddddddddd')\n",
    "]\n",
    "ppl_test_results = []\n",
    "for i, j, corpus, test_words in ppl_test_cases:\n",
    "    nm = NgramModel(i, j)\n",
    "    for text in corpus:\n",
    "        nm.update(text)\n",
    "    ppl_test_results.append(nm.perplexity(test_words))\n",
    "\n",
    "grader.grade(test_case_id = 'test_q22_ppl', answer = ppl_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kML5s78trngq"
   },
   "source": [
    "- **Answer 2.2:** Compare and discuss the perplexity for text that is similar and different from Shakespeare's plays. We provide you two dev text files, a New York Times article (`nytimes_article.txt`) and several of Shakespeare's sonnets (`shakespeare_sonnets.txt`); if you cannot find them, re-run `Setup 2: Dataset / Package` at the beginning. Also, feel free to experiment with your own text. [6 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLJHWTLXjKRC"
   },
   "source": [
    "I am surprised by the perplexity score because I thought it would be much lower expecially when comparing shakespeare to shakespeare. It was obvious that the nytimes article would be incredibly high because it is a completely different text, but the shakespeare texts were also higher than I thought. I was thinking that it should be around 2-3 instead for the bigram model since there should be very little differences to writing style. The 2+ - gram models had much greater perplexities even when i tried to adjust the smoothing. I learned that perplexity is an extemely strict metric from this. This could be important for generative ai results when we want extremely accurate information.\n",
    "\n",
    "I also included a comparison to a book called Jack Daniels Running Formula, a book about running and it resulted in an extremely high perplexity score, even higher than nytimes article. At first, I assumed that maybe its because the textbook is much longer in length, but perplexity is usually a normalized length so that cant be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qHDe_3IyYQ_F",
    "outputId": "40a817fc-8bbc-4233-80d2-cf55c2f4850c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.122231996035264"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', n=3, k=1)\n",
    "with open(\"shakespeare_sonnets.txt\", encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read()\n",
    "m.perplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k_tU7etpYUmf",
    "outputId": "0b6c0fad-352d-46e1-ca7a-b4ab58b67183"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.3881481890279685"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', n=3, k=2)\n",
    "with open(\"shakespeare_sonnets.txt\", encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read()\n",
    "m.perplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cmOP8I4mo7i",
    "outputId": "944c7ae3-5b1c-48a2-e045-320b1a010195"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.485626437020774"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', n=4, k=1)\n",
    "with open(\"shakespeare_sonnets.txt\", encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read()\n",
    "m.perplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z85JSaFOYIxZ",
    "outputId": "fbaae892-ee31-43d1-e6d2-913ad24a0a51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.689731240301278"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', n=7, k=1)\n",
    "with open(\"shakespeare_sonnets.txt\", encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read()\n",
    "m.perplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZOwEuxeY8i1",
    "outputId": "e4c45c1f-60dd-4458-af78-275952dfda12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.851819926569483"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', n=7, k=3)\n",
    "with open(\"shakespeare_sonnets.txt\", encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read()\n",
    "m.perplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAoBKahPgeJs",
    "outputId": "d45532d4-db23-4c12-aef1-349b28eebd0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.018771417967905"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModel, 'shakespeare_input.txt', n=2, k=1)\n",
    "with open(\"nytimes_article.txt\", encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read()\n",
    "m.perplexity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "TpgXe6kjo3zG"
   },
   "outputs": [],
   "source": [
    "# my own text file\n",
    "# m = create_ngram_model(NgramModel, 'shakespeare_input.txt', n=2, k=1)\n",
    "# with open(\"book.txt\", encoding='utf-8', errors='ignore') as f:\n",
    "#     text = f.read()\n",
    "# m.perplexity(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JLh_zkemppc"
   },
   "source": [
    "## 2.3 Interpolation [12 points]\n",
    "\n",
    "The idea of interpolation is to calculate the higher order n-gram probabilities also combining the probabilities for lower-order n-gram models. Like smoothing, this helps us avoid the problem of zeros if we haven't observed the longer sequence in our training data. Here's the math:\n",
    "\n",
    "$$P_{interpolation}(w_i|w_{i−2} w_{i−1}) = \\lambda_1 P(w_i|w_{i−2} w_{i−1}) + \\lambda_2 P(w_i|w_{i−1}) + \\lambda_3 P(w_i)$$\n",
    "\n",
    "where $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$.\n",
    "\n",
    "We've provided you with another class definition `NgramModelWithInterpolation` that extends `NgramModel` for you to implement interpolation. If you've written your code robustly, you should only need to override the `get_vocab(self)`, `update(self, text)`, and `prob(self, context, char)` methods, along with the initializer.\n",
    "\n",
    "The value of $n$ passed into `__init__(self, n, k)` is the highest order n-gram to be considered by the model (e.g. $n=2$ will consider 3 different length n-grams). Add-k smoothing should take place only when calculating the individual order n-gram probabilities, not when calculating the overall interpolation probability.\n",
    "\n",
    "By default set the lambdas to be equal weights, but you should also write a helper function that can be called to overwrite this default. Setting the lambdas in the helper function can either be done heuristically or by using a development set, but in the example code below, we've used the default.\n",
    "\n",
    "```python\n",
    ">>> m = NgramModelWithInterpolation(1, 0)\n",
    ">>> m.update('abab')\n",
    ">>> m.prob('a', 'a')\n",
    "0.25\n",
    ">>> m.prob('a', 'b')\n",
    "0.75\n",
    "\n",
    ">>> m = NgramModelWithInterpolation(2, 1)\n",
    ">>> m.update('abab')\n",
    ">>> m.update('abcd')\n",
    ">>> m.prob('~a', 'b')\n",
    "0.4682539682539682\n",
    ">>> m.prob('ba', 'b')\n",
    "0.4349206349206349\n",
    ">>> m.prob('~c', 'd')\n",
    "0.27222222222222225\n",
    ">>> m.prob('bc', 'd')\n",
    "0.3222222222222222\n",
    "```\n",
    "- **Problem 2.3**: implement the `NgramModelWithInterpolation` class [6 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "t0_QYpixmpWD"
   },
   "outputs": [],
   "source": [
    "class NgramModelWithInterpolation(NgramModel):\n",
    "    ''' An n-gram model with interpolation '''\n",
    "\n",
    "    def __init__(self, n, k):\n",
    "        \"\"\"Initialize with highest order n and smoothing parameter k\"\"\"\n",
    "        # Initialize equal weights for interpolation\n",
    "        # Create lower order models (orders 0 to n-1)\n",
    "        super().__init__(n, k)\n",
    "\n",
    "        self.models = []\n",
    "        for i in range(n + 1):\n",
    "            self.models.append(NgramModel(i, k))\n",
    "\n",
    "\n",
    "        self.lambdas = []\n",
    "        for i in range(n + 1):\n",
    "            self.lambdas.append(1.0 / (n + 1))\n",
    "\n",
    "    #set custom interpolation weights\n",
    "    def set_lambdas(self, new_lambdas):\n",
    "        self.lambdas = new_lambdas\n",
    "\n",
    "\n",
    "    #ovveride vocab to get all models\n",
    "    def get_vocab(self):\n",
    "        vocab = set()\n",
    "        for model in self.models:\n",
    "            vocab.update(model.get_vocab())\n",
    "        return vocab\n",
    "\n",
    "    #overide to ipdate on all models\n",
    "    def update(self, text):\n",
    "        for model in self.models:\n",
    "            model.update(text)\n",
    "\n",
    "    #interpolate all models\n",
    "    def prob(self, context, char):\n",
    "        total_prob = 0\n",
    "\n",
    "        # For each model (order 0 to n)\n",
    "        for i, model in enumerate(self.models):\n",
    "            # Use appropriate context length for each model\n",
    "            if len(context) < i:\n",
    "                # If context is shorter than needed, pad with start symbol\n",
    "                curr_context = '~' * (i - len(context)) + context\n",
    "            else:\n",
    "                # Take last i characters\n",
    "                curr_context = context[-i:] if i > 0 else ''\n",
    "\n",
    "            # Get probability from current model and apply lambda weight\n",
    "            curr_prob = model.prob(curr_context, char)\n",
    "            total_prob += self.lambdas[i] * curr_prob\n",
    "\n",
    "        return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "-obePOcmYXdN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VV16poctY3Z1",
    "outputId": "67eb4f4a-e363-4f5c-fe42-e9a8db2574c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! You earned 6/6 points. You are a star!\n",
      "\n",
      "Your submission has been successfully recorded in the gradebook.\n"
     ]
    }
   ],
   "source": [
    "# PennGrader - DO NOT CHANGE\n",
    "interpolation_test_cases = [\n",
    "    [(0, 1), ['abab'], ('a', 'a')],\n",
    "    [(2, 1), ['abab', 'abcd'], ('~c', 'd')],\n",
    "    [(2, 1), ['abeadab', 'abcd', 'eadb'], ('~a','b')],\n",
    "    [(2, 1), ['abab', 'abcd', 'aaaaad'], ('bc', 'd')],\n",
    "]\n",
    "\n",
    "interpolation_test_results = []\n",
    "for (n, k), corpus, (context, char) in interpolation_test_cases:\n",
    "    nm_interpolation = NgramModelWithInterpolation(n, k)\n",
    "    for text in corpus:\n",
    "        nm_interpolation.update(text)\n",
    "    interpolation_test_results.append(nm_interpolation.prob(context, char))\n",
    "\n",
    "grader.grade(test_case_id = 'test_q23_interpolation', answer = interpolation_test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "How1uG2iJ1gs"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzLBH9wgsDIw"
   },
   "source": [
    "- **Answer 2.3**: Experiment with a few different lambdas and values of k and discuss the effects of smoothing and interpolation in terms of perplexity. Alternatively, you can also try different number of n. You may still use text files from 2.2. [6 points]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcR7DIorpDiI",
    "outputId": "94770e76-a96a-4c1e-f58d-8ec02f2b7b2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 n=2 k = 1\n",
      "0.2372549019607843\n",
      "0.4509803921568627\n",
      "0.3058823529411765\n",
      "model 2 n=2 k = 0.5\n",
      "0.47435897435897434\n",
      "0.49633699633699635\n",
      "0.16544566544566544\n",
      "0.06837606837606837\n",
      "model 2 with lambdas n=2 k = 0.5\n",
      "0.4923076923076923\n",
      "0.5131868131868131\n",
      "0.17106227106227107\n",
      "0.07884615384615384\n",
      "model 3 n=7 k = 1\n",
      "0.45624999999999993\n",
      "0.21458333333333335\n",
      "0.4432539682539682\n",
      "0.39999999999999997\n",
      "model 4 n=7 k = 1\n",
      "0.46750000000000014\n",
      "0.21666666666666667\n",
      "0.4601587301587303\n",
      "0.40000000000000013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"model 1 n=2 k = 1\")\n",
    "m = NgramModelWithInterpolation(2, 1)\n",
    "m.update('sususususushi')\n",
    "print(m.prob('~s', 's'))\n",
    "print(m.prob('~s', 'u'))\n",
    "print(m.prob('sh', 'i'))\n",
    "print(\"model 2 n=2 k = 0.5\")\n",
    "m = NgramModelWithInterpolation(2, 0.5)\n",
    "m.update('susususushi')\n",
    "print(m.prob('~~', 's'))\n",
    "print(m.prob('~s', 'u'))\n",
    "print(m.prob('~s', 'h'))\n",
    "print(m.prob('su', 'z'))\n",
    "print(\"model 2 with lambdas n=2 k = 0.5\")\n",
    "m = NgramModelWithInterpolation(2, 0.5)\n",
    "m.set_lambdas([0.1, 0.2, 0.7])\n",
    "m.update('susususushi')\n",
    "print(m.prob('~~', 's'))\n",
    "print(m.prob('~s', 'u'))\n",
    "print(m.prob('~s', 'h'))\n",
    "print(m.prob('su', 'z'))\n",
    "print(\"model 3 n=7 k = 1\")\n",
    "m = NgramModelWithInterpolation(7, 1)\n",
    "m.update('susususushi')\n",
    "print(m.prob('~~~~su','s'))\n",
    "print(m.prob('~~~~~sh', 'z'))\n",
    "print(m.prob('~~~susus', 'u'))\n",
    "print(m.prob('~~~~~~', 's'))\n",
    "print(\"model 4 n=7 k = 1\")\n",
    "m = NgramModelWithInterpolation(7, 1)\n",
    "m.set_lambdas([0.1, 0.2, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1])\n",
    "m.update('susususushi')\n",
    "print(m.prob('~~~~su','s'))\n",
    "print(m.prob('~~~~~sh', 'z'))\n",
    "print(m.prob('~~susus', 'u'))\n",
    "print(m.prob('~~~~~~', 's'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfmaC4kXl4ia",
    "outputId": "76c96c12-e0ce-43af-caf4-8ba156598fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021487433634107544\n",
      "0.06109323584027111\n"
     ]
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModelWithInterpolation, 'shakespeare_input.txt', n=2, k=1)\n",
    "m.set_lambdas([0.1, 0.2, 0.7])\n",
    "print(m.prob('~~t','a'))\n",
    "print(m.prob('the', 'a'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QjkDKjkIv2mw",
    "outputId": "81a89797-0d82-4128-c4d5-fa030b13e9f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029105724660398395\n",
      "0.04074692612227151\n"
     ]
    }
   ],
   "source": [
    "m = create_ngram_model(NgramModelWithInterpolation, 'shakespeare_input.txt', n=2, k=1)\n",
    "m.set_lambdas([0.3, 0.3, 0.4])\n",
    "print(m.prob('~pe','o'))\n",
    "print(m.prob('~~e', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgNj8mruR4-6"
   },
   "source": [
    " I tested multiple ngrams with interpolation. It made sense that a longer length ngram model would have less probabilities since words are not that long. Increasing \"k\" smoothin would distribute some of these probabilities. The first parts of experiementing were with phrases. I picked repeatable \"susu\" so I could test the different results for ones that were uncommon such as the  \"h\" or \"i\" near the end. I wanted to see how it handles unseen characters in the second model and those look to drop to 0.1 in the bigram model. Changing the lambda values made much smaller differences than the smoothing values, but I can see its use for setting equal weights, or an increasing/decreasing lambda to adjust probabilities.\n",
    "\n",
    " I then tested on a provided txt document and found that it was extremely difficult. This is mostl ikely due to the size of the vocabulary and ngrams being unable to figure out the long range dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IHnH__4pDvT"
   },
   "source": [
    "# Section 3: Text Classification using N-Grams [20 points + 5 bonus]\n",
    "**No PennGrader in this section, see `Deliverables` for grading details**\n",
    "## Overview\n",
    "Language models can be applied to text classification. If we want to classify a text $D$ into a category $c \\in C={c_1, ..., c_N}$. We can pick the category $c$ that has the largest posterior probability given the text. That is,\n",
    "\n",
    "$$ c^* = arg max_{c \\in C} P(c|D) $$\n",
    "\n",
    "Using Bayes rule, this can be rewritten as:\n",
    "\n",
    "$$ c^* = arg max_{c \\in C} P(D|c) P(c)$$\n",
    "\n",
    "If we assume that all classes are equally likely, then we can just drop the $P(c)$ term:\n",
    "\n",
    "$$ = arg max_{c \\in C} P(D|c)$$\n",
    "\n",
    "Here $P(D \\mid c)$ is the likelihood of $D$ under category $c$, which can be computed by training language models for all texts associated with category $c$. This technique of text classification is drawn from [literature on authorship identification](http://www.aclweb.org/anthology/E/E03/E03-1053.pdf), where the approach is to learn a separate language model for each author, by training on a data set from that author. Then, to categorize a new text D, they use each language model to calculate the likelihood of D under that model, and pick the  category that assigns the highest probability to D.\n",
    "\n",
    "\n",
    "## Try it!\n",
    "We have provided you training and validation datsets consisting of the names of cities. The task is to predict the country a city is in. The following countries are including in the dataset.\n",
    "\n",
    "```\n",
    "af\tAfghanistan\n",
    "cn\tChina\n",
    "de\tGermany\n",
    "fi\tFinland\n",
    "fr\tFrance\n",
    "in\tIndia\n",
    "ir\tIran\n",
    "pk\tPakistan\n",
    "za\tSouth Africa\n",
    "```\n",
    "\n",
    "We'll set up a leaderboard for the text classification task. **Your job is to configure a set of language models that perform the best on the text classification task.** We will use the `city names` dataset, which you should have already downloaded. The test set has one unlabeled city name per line.\n",
    "\n",
    "Your code should output a file `test_labels.txt` with one two-letter country code per line.\n",
    "\n",
    "Feel free to extend the `NgramModel` or `NgramModelWithInterpolation` when creating your language model. Possible ideas to consider and experiment with when creating your model are utilizing a special end-of-text character, trying a new method for determining the vocab, and improving how your model handles novel characters.\n",
    "\n",
    "## Deliverables\n",
    "- **Answer 3.1:** Train your best model on the given training data (located in `train` folder), and report accuracy on the given validation data (located in `val` folder) for your final model. In order to receive full credit, your model must be able to outperform all the baselines. [5 points]\n",
    "\n",
    "- **Answer 3.2:** Describe the parameters you used for the final submission, such as n , k and interpolation. Discuss how you made the decision and why your combination have better results, we encourage using a table to display all validation accuracies of your experiment process. Be sure to include a detailed error analysis: identify several examples of cities on which your final model is making errors, discuss potential reasons. [15 points]\n",
    "\n",
    "- **Answer 3.3:** Make predictions on the given test data (`cities_test.txt`) and upload `test_labels.txt` to Gradescope, which contains one two-letter country code per line. Please note that testing accuracy should be valid and outperform the baseline in order to receive full credits (included in 3.1). Top 3 will be granted 5 bonus points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CerI5lKHgYeE"
   },
   "source": [
    "The classifier uses the min perplexity scores to have the most accurate matching possible. I think the flaws to this approach are cities with long names and certain prefixes. Some cities like \"Africanda\" or prefixes with \"San\" from af.txt might be mis classified because they can show up more frequently for other countries. While I was tuning parameters, I figured that the 3-gram model seemed to be giving me the best scores of around 0.65. After a little bit of tuning with smoothing, and lambdas I came to a conclusion that equal weights provided the best scores. I believe that the length of an 3 gram model is ideal since some of these city names do have 3 letter suffixes. The 2 gram model would be a little too short while the 4 gram model may be a little too specific after the pre/suffix of these cities. The smoothing factor at 0.7 in combination showed the current best accuracy mostly because of the way the letters are arranged in the names of these cities. While some cities can be predictable to Africana, some cities have names like \"dzk\" which is very uncommon. Because we have a 3gram model, I assume having equal weight lambdas allows us to capture 3 letter phrases better instead of a partial match or overshooting the phrase. It allows us to handle sequences including \"dzk\" along with \"San\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "Owpkx1JwfVa9"
   },
   "outputs": [],
   "source": [
    "COUNTRY_CODES = ['af', 'cn', 'de', 'fi', 'fr', 'in', 'ir', 'pk', 'za']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "sogf2jkI4lim"
   },
   "outputs": [],
   "source": [
    "train_dir = 'train'\n",
    "val_dir = 'val'\n",
    "test_dir = 'cities_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "K0VDIufneFUP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_models(train_dir, n, k):\n",
    "    models = {}\n",
    "    for filename in os.listdir(train_dir):\n",
    "        country_code = filename.split('.')[0]  #\n",
    "        filepath = os.path.join(train_dir, filename)\n",
    "        models[country_code] = create_ngram_model(NgramModelWithInterpolation, filepath, n, k)\n",
    "\n",
    "        with open(filepath, encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                models[country_code].update(line.strip())\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_ujQJ7FZZ5z"
   },
   "outputs": [],
   "source": [
    "def train_models(models, val_dir):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for filename in os.listdir(val_dir):\n",
    "        filepath = os.path.join(val_dir, filename)\n",
    "        val_country = filename.split('.')[0]\n",
    "\n",
    "        with open(filepath, encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                perplexities = {}\n",
    "\n",
    "                for country in models.keys():\n",
    "                    perplexities[country] = models[country].perplexity(line)\n",
    "                    pred_country = min(perplexities, key=perplexities.get)\n",
    "                y_pred.append(pred_country)\n",
    "                y_true.append(val_country)\n",
    "\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "L836UeCfaoOb"
   },
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    correct = 0\n",
    "    for i, pred in enumerate(y_pred):\n",
    "        if pred == y_true[i]:\n",
    "            correct += 1\n",
    "    accuracy = correct/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVNaRZp3WRU3"
   },
   "outputs": [],
   "source": [
    "def tune_parameters(train_dir, val_dir):\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    # for n in [2, 3, 4, 5]:\n",
    "    #     for k in [0.4, 0.55, 0.7, 1]:\n",
    "    #         lambda_options = [\n",
    "    #             [0.4, 0.3, 0.2, 0.1],\n",
    "    #             [0.25, 0.25, 0.25, 0.25],\n",
    "    #             [0.1, 0.2, 0.3, 0.4]\n",
    "    #             [0.1, 0.4, 0.4, 0.1]\n",
    "    #         ]\n",
    "\n",
    "    for n in [2, 3]:\n",
    "        for k in [0.4, 0.5, 0.7, 1]:\n",
    "            lambda_options = [\n",
    "                [0.4, 0.3, 0.2, 0.1],\n",
    "                [0.25, 0.25, 0.25, 0.25],\n",
    "                [0.1, 0.2, 0.3, 0.4]\n",
    "            ]\n",
    "\n",
    "            for lambdas in lambda_options:\n",
    "                models = get_models(train_dir, n, k)\n",
    "\n",
    "                for model in models.values():\n",
    "                    model.set_lambdas(lambdas)\n",
    "\n",
    "                y_pred, y_true = train_models(models, val_dir)\n",
    "                current_accuracy = accuracy(y_pred, y_true)\n",
    "\n",
    "                if current_accuracy > best_accuracy:\n",
    "                    best_accuracy = current_accuracy\n",
    "                    best_params = {\n",
    "                        'n': n,\n",
    "                        'k': k,\n",
    "                        'lambdas': lambdas\n",
    "                    }\n",
    "\n",
    "                print(f\"model ({n}, {k}, {lambdas}): {current_accuracy}\")\n",
    "\n",
    "    return best_params, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzjdOVbSWX-3",
    "outputId": "1fb12756-e900-45aa-b936-67689f672537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model (n, k, [lambdas])\n",
      "model (2, 0.4, [0.4, 0.3, 0.2, 0.1]): 0.6677777777777778\n",
      "model (2, 0.4, [0.25, 0.25, 0.25, 0.25]): 0.6677777777777778\n",
      "model (2, 0.4, [0.1, 0.2, 0.3, 0.4]): 0.6633333333333333\n",
      "model (2, 0.5, [0.4, 0.3, 0.2, 0.1]): 0.6655555555555556\n",
      "model (2, 0.5, [0.25, 0.25, 0.25, 0.25]): 0.6655555555555556\n",
      "model (2, 0.5, [0.1, 0.2, 0.3, 0.4]): 0.6666666666666666\n",
      "model (2, 0.7, [0.4, 0.3, 0.2, 0.1]): 0.6666666666666666\n",
      "model (2, 0.7, [0.25, 0.25, 0.25, 0.25]): 0.6644444444444444\n",
      "model (2, 0.7, [0.1, 0.2, 0.3, 0.4]): 0.6644444444444444\n",
      "model (2, 1, [0.4, 0.3, 0.2, 0.1]): 0.6666666666666666\n",
      "model (2, 1, [0.25, 0.25, 0.25, 0.25]): 0.6711111111111111\n",
      "model (2, 1, [0.1, 0.2, 0.3, 0.4]): 0.6688888888888889\n",
      "model (3, 0.4, [0.4, 0.3, 0.2, 0.1]): 0.6855555555555556\n",
      "model (3, 0.4, [0.25, 0.25, 0.25, 0.25]): 0.6933333333333334\n",
      "model (3, 0.4, [0.1, 0.2, 0.3, 0.4]): 0.7\n",
      "model (3, 0.5, [0.4, 0.3, 0.2, 0.1]): 0.6844444444444444\n",
      "model (3, 0.5, [0.25, 0.25, 0.25, 0.25]): 0.6911111111111111\n",
      "model (3, 0.5, [0.1, 0.2, 0.3, 0.4]): 0.7033333333333334\n",
      "model (3, 0.7, [0.4, 0.3, 0.2, 0.1]): 0.6822222222222222\n",
      "model (3, 0.7, [0.25, 0.25, 0.25, 0.25]): 0.69\n",
      "model (3, 0.7, [0.1, 0.2, 0.3, 0.4]): 0.7055555555555556\n",
      "model (3, 1, [0.4, 0.3, 0.2, 0.1]): 0.6822222222222222\n",
      "model (3, 1, [0.25, 0.25, 0.25, 0.25]): 0.6944444444444444\n",
      "model (3, 1, [0.1, 0.2, 0.3, 0.4]): 0.7022222222222222\n",
      "best accuracy: 0.7055555555555556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"model (n, k, [lambdas])\")\n",
    "best_params, best_acc = tune_parameters(train_dir, val_dir)\n",
    "print(\"best accuracy:\", best_acc)\n",
    "\n",
    "models = get_models(train_dir, best_params['n'], best_params['k'])\n",
    "for model in models.values():\n",
    "    model.set_lambdas(best_params['lambdas'])\n",
    "\n",
    "y_pred, y_true = train_models(models, val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "sJc1Z8O3hOFq"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "with open(test_dir, encoding = 'utf-8', errors = 'ignore') as f:\n",
    "    for line in f:\n",
    "        perplexities = {}\n",
    "        for country in models.keys():\n",
    "            perplexities[country] = models[country].perplexity(line)\n",
    "        pred_country = min(perplexities, key = perplexities.get)\n",
    "        results.append(pred_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "M9suMGVznH16"
   },
   "outputs": [],
   "source": [
    "with open('test_labels.txt', 'w') as f:\n",
    "    for label in results:\n",
    "        f.write(str(label)+ '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3LdTWmEshYa"
   },
   "source": [
    "# Submission\n",
    "### Congratulation on finishing your homework! Here are the deliverables you need to submit to GradeScope\n",
    "- This notebook and py file: rename to `homework3.ipynb` and `homework3.py`. You can download the notebook and py file by going to the top-left corner of this webpage, `File -> Download -> Download .ipynb/.py`\n",
    "- `test_labels.txt` from `Section 3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "5dTscRWLRyrm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
